"""LLM-based rule generation: bootstrap from documents + incremental learning."""

from __future__ import annotations

import logging
import re
import uuid
from typing import TYPE_CHECKING, Any

from pydantic import BaseModel, Field

from silicon_memory.entities.types import DetectorRule, ExtractorRule

if TYPE_CHECKING:
    from silicon_memory.llm.provider import SiliconLLMProvider

logger = logging.getLogger(__name__)

_ALIAS_PATTERN = re.compile(r"(\w+(?:\s+\w+)*)\s*\(([^)]+)\)")


class ExtractedEntity(BaseModel):
    """An entity found by the LLM in sample text."""

    text: str
    entity_type: str
    canonical: str
    context: str


class ExtractResult(BaseModel):
    """LLM response for entity extraction."""

    entities: list[ExtractedEntity] = Field(default_factory=list)


class GeneratedRule(BaseModel):
    """A single rule generated by the LLM."""

    detector_pattern: str
    detector_description: str
    extractor_pattern: str
    entity_type: str
    normalize_template: str
    context_examples: list[str] = Field(default_factory=list)


class RulesResult(BaseModel):
    """LLM response for rule generation."""

    rules: list[GeneratedRule] = Field(default_factory=list)


_EXTRACT_PROMPT = """\
Analyze this text and find all entity references — names, IDs, codes, \
law references, case numbers, project identifiers, section references, \
or any structured identifier.

For each entity found, provide:
- text: the exact text as it appears
- entity_type: a short category (e.g. "law_ref", "case_id", "person", "project_id")
- canonical: a normalized canonical form
- context: ~50 characters surrounding the reference

Text:
{text}

Respond with JSON: {{"entities": [...]}}"""

_RULES_PROMPT = """\
Given these entity examples extracted from documents, generate regex rules \
to detect similar entities in new text.

For each entity type, create a JSON object with EXACTLY these fields:
- "detector_pattern": broad regex string (high recall, catches all similar patterns)
- "detector_description": short human-readable description of what it detects
- "extractor_pattern": precise regex string with capture groups for normalization
- "entity_type": the entity type string (e.g. "case_id", "person", "law_ref")
- "normalize_template": template using {{match}}, {{group1}}, {{group2}}, \
{{entity_type}}, {{match_lower}}, {{match_upper}}
- "context_examples": array of 3-5 example phrases showing typical usage

Entity examples:
{examples}

Respond with ONLY valid JSON (no Python syntax, no r-strings, no code): \
{{"rules": [{{...}}, ...]}}

IMPORTANT: All regex patterns must be valid JSON strings. Use double \
backslashes for regex escapes (e.g. "\\\\d+" not r"\\d+", "\\\\s+" not r"\\s+"). \
Use non-capturing groups (?:...) where appropriate. Each extractor_pattern \
must have at least one capture group."""


class RuleLearner:
    """LLM-based rule generation for entity resolution.

    Two modes:
    - bootstrap(): Analyze sample documents → extract entities → generate rules
    - generate_rules(): Generate rules from already-collected examples
    """

    def __init__(self, llm: "SiliconLLMProvider") -> None:
        self._llm = llm

    async def extract_entities(self, text: str) -> list[dict[str, Any]]:
        """Use LLM to find all entity references in text."""
        result = await self._llm.generate_structured(
            _EXTRACT_PROMPT.format(text=text[:3000]),
            ExtractResult,
            max_tokens=4096,
        )
        return [e.model_dump() for e in result.entities]

    async def generate_rules(
        self, examples: list[dict[str, Any]]
    ) -> tuple[list[DetectorRule], list[ExtractorRule]]:
        """Generate detector + extractor rules from entity examples."""
        examples_text = "\n".join(
            f"- {e.get('text', '')} [{e.get('entity_type', '')}] "
            f"context: {e.get('context', '')}"
            for e in examples
        )
        result = await self._llm.generate_structured(
            _RULES_PROMPT.format(examples=examples_text),
            RulesResult,
            max_tokens=4096,
        )

        detectors: list[DetectorRule] = []
        extractors: list[ExtractorRule] = []

        for rule in result.rules:
            rule_id = uuid.uuid4().hex[:8]
            try:
                re.compile(rule.detector_pattern)
            except re.error:
                logger.warning("Invalid detector regex from LLM: %s", rule.detector_pattern)
                continue
            try:
                re.compile(rule.extractor_pattern)
            except re.error:
                logger.warning("Invalid extractor regex from LLM: %s", rule.extractor_pattern)
                continue

            detectors.append(DetectorRule(
                id=f"d_{rule_id}",
                pattern=rule.detector_pattern,
                description=rule.detector_description,
            ))
            extractors.append(ExtractorRule(
                id=f"e_{rule_id}",
                entity_type=rule.entity_type,
                detector_ids=[f"d_{rule_id}"],
                pattern=rule.extractor_pattern,
                normalize_template=rule.normalize_template,
                context_examples=rule.context_examples,
            ))

        return detectors, extractors

    async def bootstrap(
        self, text: str
    ) -> tuple[list[DetectorRule], list[ExtractorRule], list[tuple[str, str]]]:
        """Full bootstrap: text → extract entities → generate rules → discover aliases.

        Returns (detectors, extractors, aliases).
        """
        examples = await self.extract_entities(text)
        if not examples:
            return [], [], []

        detectors, extractors = await self.generate_rules(examples)
        aliases = self.discover_aliases(text)

        return detectors, extractors, aliases

    @staticmethod
    def discover_aliases(text: str) -> list[tuple[str, str]]:
        """Detect parenthetical aliases — no LLM needed.

        "arbeidsmiljøloven (aml.)" → ("aml.", "arbeidsmiljøloven")
        Returns list of (short_form, long_form) tuples.
        """
        results: list[tuple[str, str]] = []
        for m in _ALIAS_PATTERN.finditer(text):
            long_form = m.group(1).strip()
            short_form = m.group(2).strip()
            if len(short_form) < len(long_form):
                results.append((short_form, long_form))
            else:
                results.append((long_form, short_form))
        return results
